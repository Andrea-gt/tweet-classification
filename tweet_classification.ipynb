{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LM35_usH8d"
      },
      "source": [
        "# **Natural Language Processing with Disaster Tweets ‚õà**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEzwnzhhsjCh"
      },
      "source": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin semper, sem ut pulvinar pulvinar, est leo pretium sem, vel varius erat orci quis turpis. Aliquam augue orci, semper at mi vel, auctor fermentum ipsum. Nulla hendrerit hendrerit consequat. Vestibulum interdum, dolor id ultricies hendrerit, leo purus dignissim mauris, sit amet maximus nunc mauris eu mi. Quisque hendrerit ante eget pharetra iaculis. Nunc ullamcorper ante quam, non laoreet urna cursus eget. In hac habitasse platea dictumst. Donec ac risus consectetur, euismod magna sit amet, elementum justo. Sed non nisl lacinia, gravida odio ac, vehicula felis. Morbi non accumsan diam. Aenean nibh nunc, pulvinar quis vestibulum ac, tincidunt efficitur nibh. Maecenas vitae molestie leo. Pellentesque varius at felis vitae tristique. Nunc rutrum nisl sagittis velit finibus congue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_jC5FR9sTsO"
      },
      "source": [
        "**Authors:**\n",
        "- [Andrea Ramirez](https://github.com/Andrea-gt)\n",
        "- [Adrian Flores](https://github.com/adrianRFlores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGnNhm6-scLp"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5QxOsI1skVI"
      },
      "source": [
        "## **(1) Import Libraries** ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gZxx2TueD9PK"
      },
      "outputs": [],
      "source": [
        "#!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x2NYNwFWr4tK"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import kurtosis, skew, probplot\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "import itertools\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Standard libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ===== ===== Reproducibility Seed ===== =====\n",
        "# Set a fixed seed for the random number generator for reproducibility\n",
        "random_state = 42\n",
        "\n",
        "# Set matplotlib inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Set default figure size\n",
        "plt.rcParams['figure.figsize'] = (6, 4)\n",
        "\n",
        "# Define custom color palette\n",
        "palette = sns.color_palette(\"viridis\", 12)\n",
        "\n",
        "# Set the style of seaborn\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tiOG7y3s9Z5"
      },
      "source": [
        "## **(2) Data Upload** üìÑ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XdvjmV7Ws7QM",
        "outputId": "92365861-4cd7-4bae-c14f-4f3e88ac35f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('data/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCuBbjcatPpq"
      },
      "source": [
        "## **(3) Exploratory Analysis** üîé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcgsVes9vEic"
      },
      "source": [
        "### **(1) Descripci√≥n General de los Datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99alb-YetMwG",
        "outputId": "e66a3041-df1d-4726-d11f-d34ec46b5220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The given dataset has 7613 registers and 5 columns.\n"
          ]
        }
      ],
      "source": [
        "# Print the number of records in the DataFrame\n",
        "print(\"The given dataset has\", df.shape[0], \"registers and\", df.shape[1], \"columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6WTPUnCtW9j"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuyQOccguBoF"
      },
      "source": [
        "> - El conjunto de datos original cuenta con 7613 registros y 5 columnas, lo que indica que tiene una dimensi√≥n relativamente peque√±a. Cada uno de los 7613 registros representa una observaci√≥n √∫nica, mientras que las 5 columnas corresponden a diferentes caracter√≠sticas o variables medidas para cada observaci√≥n, incluyendo el texto de un tweet, una palabra clave asociada y la ubicaci√≥n desde donde se envi√≥, aunque estas √∫ltimas dos pueden estar en blanco en algunas ocasiones.\n",
        "\n",
        "**Fuente:** [P√°gina oficial de Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/data?select=train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setsXYbstXOb",
        "outputId": "dcf79400-fd27-472b-8a03-17688647bf84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# Basic information about the dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9K0yVGhurKq"
      },
      "source": [
        "Como se puede observar, se cuentan con 5 columnas o features en este conjunto de datos, siendo estas las que se describen a continuaci√≥n.\n",
        "\n",
        "- **id**: un identificador √∫nico para cada tweet.\n",
        "- **text**: el texto del tweet.\n",
        "- **location**: la ubicaci√≥n desde donde se envi√≥ el tweet (puede estar en blanco).\n",
        "- **keyword**: una palabra clave particular del tweet (puede estar en blanco).\n",
        "- **target**: en el archivo `train.csv` solamente, indica si un tweet es sobre un desastre real (`1`) o no (`0`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfdyoDvvAj0"
      },
      "source": [
        "### **(2) Clasificaci√≥n de las Variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIMGpDKevUxe"
      },
      "source": [
        "| **Nombre**    | **Descripci√≥n**                                             | **Tipo de variable**                     |\n",
        "|---------------|-------------------------------------------------------------|------------------------------------------|\n",
        "| **id**        | Un identificador √∫nico para cada tweet                      | Cuantitativa                             |\n",
        "| **text**      | El texto del tweet                                          | Cualitativa (descriptiva)                |\n",
        "| **location**  | La ubicaci√≥n desde donde se envi√≥ el tweet                  | Cualitativa (descriptiva)                |\n",
        "| **keyword**   | Una palabra clave particular del tweet                      | Cualitativa (descriptiva)                |\n",
        "| **target**    | Indica si un tweet es sobre un desastre real o no           | Cualitativa (binaria)                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pwtYuICvqLV"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Aof5_0mvqoF"
      },
      "source": [
        "> - El conjunto de datos posee 3 variables cualitativas descriptivas y 1 de tipo binaria.\n",
        "> - La √∫ltima variable del conjunto de datos (id) es de tipo cuantitativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBnYap4rwAB9"
      },
      "source": [
        "### **(3) Exploraci√≥n y Limpieza Inicial de los Datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRTaGCsd-zA3"
      },
      "source": [
        "#### **(1) Preprocesamiento de los Datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tAt0ltErBDbP"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcY-E1d6DgXQ",
        "outputId": "35b65921-9f64-4a2f-a28a-0b4ab02d541d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the NLTK stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "# Initialize the PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QGRNdJOLDi_p"
      },
      "outputs": [],
      "source": [
        "# Get the list of stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PpBl5wyvDY3g"
      },
      "outputs": [],
      "source": [
        "# Function to remove stopwords and apply stemming\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "    # Remove stopwords and apply stemming\n",
        "    processed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    # Reassemble the text\n",
        "    return ' '.join(processed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hT30vQ1sBZrw"
      },
      "outputs": [],
      "source": [
        "# Convert all entries to strings\n",
        "df['text'] = df['text'].astype(str)\n",
        "# Remove URLs\n",
        "df['text'] = df['text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False, regex=True)\n",
        "# Convert to lowercase\n",
        "df['text'] = df['text'].str.lower()\n",
        "# Remove leading/trailing whitespaces\n",
        "df['text'] = df['text'].str.strip()\n",
        "# Remove special characters and punctuation (keeping letters, numbers, and spaces)\n",
        "df['text'] = df['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "# Remove extra spaces\n",
        "df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n",
        "# Apply preprocessing (stopwords removal and stemming)\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "ytCCddUOlm0L",
        "outputId": "02a3a7c8-20f4-40ce-a713-d21327c6e1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qinNeP9SXhb-",
        "outputId": "fa827aba-5b5c-484c-d409-a0c5e4863e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Locations:\n",
            "\n",
            "Total number of unique locations: 3342\n"
          ]
        }
      ],
      "source": [
        "# Converting the column to a list\n",
        "column_to_list = df['location'].unique().tolist()\n",
        "# Convert all elements to strings to avoid TypeError\n",
        "column_to_list_str = [str(location) for location in column_to_list]\n",
        "# Pretty print the list and its length\n",
        "#print(\"Unique Locations:\")\n",
        "# ==== WARNING: The output below is big. ====\n",
        "# This has been commented out to avoid overwhelming the noebook with too much data.\n",
        "# print(\", \".join(column_to_list_str))\n",
        "print(f\"\\nTotal number of unique locations: {len(column_to_list_str)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B9LCLX6_lt5s"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['location'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s2BkqALtc3tU"
      },
      "outputs": [],
      "source": [
        "# Drop rows where 'keyword' column has NaN values\n",
        "df = df.dropna(subset=['keyword'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5WetphX_ag8",
        "outputId": "6b392579-74ff-4046-8e7d-53124f037462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The given dataset has 6940 registers and 3 columns.\n"
          ]
        }
      ],
      "source": [
        "# Check duplicate rows in dataset\n",
        "df = df.drop_duplicates()\n",
        "# Print the number of records in the DataFrame\n",
        "print(\"The given dataset has\", df.shape[0], \"registers and\", df.shape[1], \"columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWohK8wjado"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceMyJ4FXja_c"
      },
      "source": [
        "> - Primero eliminaremos la columna `id`, ya que no aporta informaci√≥n significativa al conjunto de datos. Dado que cada registro corresponde a una observaci√≥n √∫nica, esta columna no contribuye a la variabilidad o al an√°lisis. Al eliminarla, reducimos la dimensionalidad, lo que puede mejorar la eficiencia de nuestro algoritmo y simplificar la interpretaci√≥n posterior de este."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJmfht5QYxPI"
      },
      "source": [
        "> - Continuamos el preprocesamiento con dos pasos cruciales para optimizar el an√°lisis de texto: la aplicaci√≥n de un stemmer y la eliminaci√≥n de stopwords (o palabras vac√≠as)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KImIzJY4Y0kZ"
      },
      "source": [
        "\n",
        "> - Contin√∫a el preprocesamiento con la realizaci√≥n de varias transformaciones para limpiar y estandarizar el texto en la columna text. Primero, convertimos todas las entradas a cadenas de texto para asegurar la consistencia en el tipo de dato. A continuaci√≥n, eliminamos URLs, que suelen introducir ruido sin aportar valor al an√°lisis. Tambi√©n convertimos el texto a min√∫sculas y eliminamos los espacios en blanco al inicio y al final de las cadenas para mantener uniformidad. Luego, removemos caracteres especiales y signos de puntuaci√≥n, dejando solo letras, n√∫meros y espacios, lo que facilita el an√°lisis posterior. Adem√°s, eliminamos espacios extra y normalizamos el texto quitando acentos.\n",
        "\n",
        "> - La columna location presenta una cantidad considerable de datos faltantes, aproximadamente un 35% de las entradas son nulas. Adem√°s, estas ausencias no son uniformes, ya que algunas se representan con s√≠mbolos como '???' o contienen caracteres especiales que complican a√∫n m√°s su interpretaci√≥n. Esta falta de consistencia en los datos, sumada al hecho de que la variable de ubicaci√≥n podr√≠a introducir sesgos significativos en el modelo, nos lleva a concluir que es mejor eliminar esta columna.\n",
        "\n",
        "> - Seguidamente se realiza imputaciones en la columna `keyword` al eliminar las entradas nulas, esto se justifica con el hecho de que estas entradas representan √∫nicamente alrededor de un 0.9% de nuestros datos.\n",
        "\n",
        "> - Como √∫ltimo paso del preprocesamiento, filtramos los valores duplicados en el conjunto de datos. Al eliminar duplicados, aseguramos que cada registro sea √∫nico, lo que mejora la integridad de los datos y optimiza la precisi√≥n del modelo al trabajar con un conjunto no redundante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWLvGawUY-SI"
      },
      "source": [
        "**Nota:**\n",
        "\n",
        "  1.   El **stemmer** reduce las palabras a su ra√≠z o forma b√°sica, lo que permite agrupar diferentes variaciones de una misma palabra bajo una √∫nica representaci√≥n. Esto no solo disminuye la dimensionalidad del conjunto de datos haciendo que sea m√°s f√°cil procesar este, sino que tambi√©n mejora la capacidad de los algoritmos para identificar patrones relevantes en el texto. [[Referencia]](https://www.geeksforgeeks.org/introduction-to-stemming/)\n",
        "\n",
        "  2.   Por otro lado, la eliminaci√≥n de **stopwords** filtra palabras comunes que, aunque frecuentes, aportan poco valor sem√°ntico al an√°lisis, como \"y\", \"el\", \"en\", entre otras. Al excluir estas palabras, se enfoca el modelo en t√©rminos m√°s significativos, lo que puede resultar en una mejora notable en la precisi√≥n y eficiencia del an√°lisis textual. Sin embargo, el beneficio m√°s grande es la reducci√≥n de dimensionalidad, permitiendo que el entrenamiento sea m√°s r√°pido. [[Referencia]](https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH32JTVm_U6l"
      },
      "source": [
        "#### **(2) Exploraci√≥n de los Datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJEpH9RKwC_q",
        "outputId": "952010f8-ee65-4334-ee4a-65df6b71b5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set: Text Length Statistics\n",
            "count    6940.000000\n",
            "mean       58.801729\n",
            "std        22.872899\n",
            "min         4.000000\n",
            "25%        42.000000\n",
            "50%        60.000000\n",
            "75%        76.000000\n",
            "max       127.000000\n",
            "Name: text, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Calculate the length of text entries in the 'text' column.\n",
        "length = df[\"text\"].apply(len)\n",
        "# Display descriptive statistics of text lengths.\n",
        "print(\"Training Set: Text Length Statistics\")\n",
        "print(length.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yXpT2HwlYkL"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aoFOx6dlZZ8"
      },
      "source": [
        "> - Este peque√±o an√°lisis de la longitud de los textos en el conjunto de datos nos permite descubrir varias caracter√≠sticas interesantes. Con un total de 7,001 registros, la longitud promedio de los textos es de aproximadamente 59 caracteres, lo que indica que la mayor√≠a de las entradas son relativamente cortas. La desviaci√≥n est√°ndar es de aproximadamente 23 caracteres, lo que sugiere una variabilidad moderada en la longitud de los textos.\n",
        "\n",
        "> - El rango de las longitudes var√≠a significativamente, desde un m√≠nimo de 3 caracteres hasta un m√°ximo de 127 caracteres. El 50% de los textos tienen una longitud de 60 caracteres o menos, con el 25% de los textos por debajo de 41 caracteres y el 75% por debajo de 76 caracteres. Por ende, aunque la mayor√≠a de los textos tienen una longitud similar, hay una presencia de textos mucho m√°s cortos o m√°s largos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LSx7c38-BmL",
        "outputId": "861d9684-68ea-4990-ce12-6616acc40be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 most frequent values for column 'keyword':\n",
            "1     fatalities                     45        \n",
            "2     deluge                         42        \n",
            "3     armageddon                     42        \n",
            "4     damage                         41        \n",
            "5     harm                           41        \n",
            "6     evacuate                       40        \n",
            "7     fear                           40        \n",
            "8     body%20bags                    40        \n",
            "9     twister                        40        \n",
            "10    siren                          40        \n",
            "\n",
            "===============================================\n",
            "Top 10 most frequent values for column 'text':\n",
            "1     angri woman openli accus nema steal relief materi meant idp angri intern displac wom 2         \n",
            "2     cindi noonancindynoonanheartbreak baltimor riot yahistor undergroundrailraod 2         \n",
            "3     feel like sink low selfimag take quiz 2         \n",
            "4     horribl sink feel you√ª¬™v home phone realis 3g whole time 2         \n",
            "5     break obama offici gave muslim terrorist weapon use texa attack 2         \n",
            "6     trafford centr film fan angri odeon cinema evacu follow fals fire alarm 2         \n",
            "7     new evacu order 25 home danger hwi 8 fire near roosevelt wash koin6new 2         \n",
            "8     drunk meal 101 cook your total obliter 2         \n",
            "9     wacko like michelebachman predict world soon obliter burn firey inferno cant accept globalwarm hello 2         \n",
            "10    choke hazard prompt recal kraft chees singl 2         \n",
            "\n",
            "===============================================\n",
            "Top 10 most frequent values for column 'target':\n",
            "1     0                              4105      \n",
            "2     1                              2835      \n",
            "\n",
            "===============================================\n"
          ]
        }
      ],
      "source": [
        "# See what are the 10 most frequent values for each of the dataframe columns\n",
        "for column in df.columns:\n",
        "    frequency_values = df[column].value_counts().head(10)\n",
        "    print(\"Top 10 most frequent values for column '{}':\".format(column))\n",
        "    for index, (value, frequency) in enumerate(frequency_values.items(), start=1):\n",
        "        print(\"{:<5} {:<30} {:<10}\".format(index, value, frequency))\n",
        "    print(\"\\n===============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFP8XPElbo1F"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjjDvk7Rbqmc"
      },
      "source": [
        "> - Como es posible observar, nuestras tablas de frecuencia muestran que el conjunto de datos est√° desbalanceado en t√©rminos de la variable 'target': el **valor 0** (que indica la ausencia de un desastre) aparece con mucha m√°s frecuencia (4105 casos) que el **valor 1** (que indica la presencia de un desastre) con 2835 casos. Este desbalance podr√≠a afectar el rendimiento de nuestro modelo pero primero evaluaremos su desempe√±o antes de tomar decisiones.\n",
        "\n",
        "> -  Para la columna keyword, esta simple tabla de frecuencia no nos dice lo suficiente as√≠ que haremos un an√°lisis m√°s profundo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrH-1AupaTFv",
        "outputId": "d2e87bd4-55cc-4a48-dc05-c2e6db3a8151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 most frequent keywords for target '0':\n",
            "1     body%20bags                    39        \n",
            "2     harm                           37        \n",
            "3     armageddon                     37        \n",
            "4     deluge                         36        \n",
            "5     ruin                           36        \n",
            "6     wrecked                        36        \n",
            "7     twister                        35        \n",
            "8     fear                           35        \n",
            "9     siren                          35        \n",
            "10    panic                          34        \n",
            "\n",
            "===============================================\n",
            "Top 10 most frequent keywords for target '1':\n",
            "1     evacuated                      31        \n",
            "2     debris                         31        \n",
            "3     earthquake                     30        \n",
            "4     derailment                     29        \n",
            "5     wildfire                       29        \n",
            "6     nuclear%20disaster             28        \n",
            "7     suicide%20bombing              28        \n",
            "8     evacuation                     28        \n",
            "9     buildings%20on%20fire          27        \n",
            "10    mass%20murder                  27        \n"
          ]
        }
      ],
      "source": [
        "# Pairing 0's with their most frequent keywords\n",
        "target_0_keywords = df[df['target'] == 0]['keyword'].value_counts().head(10)\n",
        "print(\"Top 10 most frequent keywords for target '0':\")\n",
        "for index, (keyword, frequency) in enumerate(target_0_keywords.items(), start=1):\n",
        "    print(f\"{index:<5} {keyword:<30} {frequency:<10}\")\n",
        "print(\"\\n===============================================\")\n",
        "\n",
        "# Pairing 1's with their most frequent keywords\n",
        "target_1_keywords = df[df['target'] == 1]['keyword'].value_counts().head(10)\n",
        "print(\"Top 10 most frequent keywords for target '1':\")\n",
        "for index, (keyword, frequency) in enumerate(target_1_keywords.items(), start=1):\n",
        "    print(f\"{index:<5} {keyword:<30} {frequency:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGCKvGKhPPs"
      },
      "source": [
        "**Observaciones üí° -->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwHBEF3ihP_O"
      },
      "source": [
        "> - Se evidencia que para el target 0, los t√©rminos m√°s frecuentes como \"body bags\" \"harm\" y \"armageddon\" est√°n relacionados con eventos graves o de alta magnitud, pero en el contexto de la ausencia de un desastre, estos podr√≠an estar mal clasificados o reflejar un contexto de preocupaci√≥n general. Por otro lado, para el target 1, palabras como \"evacuated\" \"debris\" y \"earthquake\" se relacionan claramente con eventos de emergencia reales."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
